[ { "title": "[AI 부캠 4기] week1 AI math 정리", "url": "/posts/week1math/", "categories": "Boostcamp, study", "tags": "Boostcamp, AI, math", "date": "2022-09-23 00:00:00 +0900", "snippet": "1주차에 진행된 Python 강의에 이어 AI math를 정리해보려고 합니다.역시 수학이 가장 힘들고 어려운 길이네요.저는 수학을 싫어하지는 않지만 확률과 통계에 대한 이해가 매우 부족한 편입니다.따라서 이 포스트를 보시는 분들은 잘못된 부분이 있을 수 있으니 조심하셔야 합니다.부스트 캠프의 강의보단 제가 배우고 이해한 스토리대로 나가겠습니다.왜 수학을 배우는가AI란?AI란 무엇일까요?저는 입력을 넣으면 원하는 결과가 나오는 튜링머신이 먼저 생각납니다.사람의 생각이 튜링머신이라면 AI는 그 튜링머신을 따라가려는 존재입니다.그런데 그 누구도 모든 사람의 생각을 코드로 짤 수 없습니다.강아지와 고양이를 구별하는 일반적인 알고리즘이 존재하지 않는 것처럼 말이죠.그러면 AI는 어떻게 사람의 생각을 따라갈까요?사람의 생각 또한 학습의 결과이기 때문에 AI 또한 학습으로 발전합니다.수많은 데이터를 보고 학습하여 튜링머신과 비슷한 결과 값을 내도록 말이죠.학습이란?그러면 학습이란 무엇일까요?저는 부모님과 선생님, 교수님들을 통해 이전의 역사를 외우고 익혔습니다.컴퓨터에서 학습이란 입력된 데이터들의 결과값이 누적되어 다음 입력 데이터에 대해서 누적된 결과값을 기반으로 해석, 원하던 결과를 내보낼 수 있게 하는 수단이라고 생각합니다.여기에서 AI의 학습에 대해 여러가지 방법이 있습니다. 지도 학습 : 정답이 정해져 있는 학습 문제에 대한 답이 정해져 있어서 실행 결과와 정답의 오차를 비교 비지도 학습: 정답이 없는 학습 데이터의 규칙성을 찾는다 강화 학습: 상태가 주어지는 학습 사람이 시행착오를 겪으며(정답이 없음) 학습하는 것과 유사 지도 학습 뿐만 아니라 다른 학습들의 조건들을 보면 컴퓨터 코드로 적기 애매한 부분만 있습니다.여기서 제가 주목한 부분은 오차 입니다. 오차란 수치로 표현될 수 있는 값입니다.컴퓨터는 결국 모든 것을 0과 1로 인식하기 때문에 사진, 문자열, 상태 등 모든 것을 숫자로 인식합니다.그렇다면 학습의 결과물과 사람이 원하는 결과물 또한 숫자로 인식할 수 있습니다.즉 오차를 구하기 위해선 숫자의 나열로 된두 결과값의 차이, 즉 두 벡터의 거리를 계산하면 됩니다. l1-norm, l2-norm 등의 계산법이 있습니다.\\[||x||_n = (\\sum^n_{i=1}|x_i|^p)^{1/p}\\]두 결과물의 차이를 오차라고 둔다면 AI의 최종 목표는 이 오차를 최소화 하는 것이라고 볼 수 있겠습니다.오차는 어떻게 최소화 하는가?오차를 그러면 어떻게 최소화 할까요?여러 데이터를 넣다 보면 오차가 아래 처럼 그려질 수도 있습니다.그래프에서 X는 우리가 입력할 데이터, Y는 오차라고 합시다.\\[y = (x-4) ^ 2 + 10\\]위의 수식에서 우리는 x가 4일때 최솟값이 10이고 라는 것을 알 수 있습니다.하지만 수식을 모른다고 할때, 그리고 실수는 무한하기에 그래프의 전체 모양을 잡는다던가 특정 범위의 그래프만 그릴 수는 없습니다.그러면 최솟값이 되는 x를 찾기 위해서는 어떻게 할까요? 운 없어서 10이라는 x값을 넣고도 4라는 x를 도출해 낼 수 있을까요?\\[y' = 2(x-4)\\]고등학교에서 배운 미분을 사용해보면 x = 4에서 미분값이 0이라는 것을 알 수 있습니다.그리고 x = 10 에서 미분값을 구하게 된다면 12라는 양수가 나오게 되지요.즉 미분값이 0이 되도록 x를 이동시키면 되는 것입니다!이를 경사하강법이라고 합니다. 경사를 계속 낮추어서 0에 가깝게 만드는 것이죠.경사하강법에서 x에 대한 미분값을 그래디언트라고 합니다.경사하강법경사하강법은 코드로 작성하면 아래와 같습니다.# gradient : 미분을 계산하는 함수# init : 시작점 , lr: 학습률, eps : 알고리즘 종료 조건var = initgrad = gradient(var)while(abs(grad) &gt; eps): var = var - lr * grad grad = gradient(var)수학보다는 역시 코드가 더 보기 편하네요한눈에 보기 어렵지만 그래디언트과 학습률를 곱한 것으로 입력값을 변경해서 찾아내는 것입니다.학습률이 높으면 더 빨리 찾을 수 있지만 너무 큰 값은 최소점을 지나칠 수 있기 떄문에 적당한 학습률을 사용해야 합니다.종료조건은 왜 필요할까요? 위 그래프처럼 그래디언트가 0이 되는 위치가 정수 또는 유한 소수면 문제 없겠지만 무한 소수라면 컴퓨터의 한계로 인해 절대 그래디언트이 0이 되는 x를 찾을 수 없습니다. 변수가 벡터인 경우 오차를 이야기 할때 벡터 이야기를 꺼낸적이 있었는데, 벡터에 해당하는 그래프를 다항 함수로 해석할 수 있습니다.\\[f(x, y, z) = a(x-4)^2 + b(y-2)^2 + c(z-3)^2 + d\\] 보다시피 x = 4, y = 2, z = 3일때 최솟값 d가 나오는 것을 알 수 있습니다. 경사하강법을 적용 하려면 어떻게 해야 할까요? x에 대한 최솟값을 위해 x로 미분, y에 대한 최솟값을 위해 y로 미분… 즉 각 원소에 대해 함수를 미분하여 경사하강법을 적용하면 됩니다. 이러한 미분을 편미분이라고 하며, 편미분한 함수가 모인 벡터를 그래디언트 벡터라고 합니다. var = var - lr * grad 아까전 코드에서 lr을 이제 변수인 벡터로 대응하고 grad를 그래디언트 벡터로 대체하면 경사하강법이 동일하게 동작하겠죠? 그래서 아래와 같은 코드로 표현이 됩니다. # gradient : 미분을 계산하는 함수 # init : 시작점 , lr: 학습률, eps : 알고리즘 종료 조건 var = init grad = gradient(var) while(norm(grad) &gt; eps): var = var - lr * grad grad = gradient(var) 상단의 코드와 동일하지만 while(norm(grad) &gt; eps):이라는 부분이 달라졌습니다. 벡터에서 거리는 ln-norm으로 측정하기 때문입니다. 처음에 이야기 했었죠? 선형 회귀 분석" }, { "title": "[AI 부캠 4기] week1 Python 정리", "url": "/posts/week1py/", "categories": "Boostcamp, study", "tags": "Boostcamp, Python", "date": "2022-09-22 00:00:00 +0900", "snippet": "1주차에 진행된 Python 강의를 정리해보려고 합니다.주로 모르는 것 위주로만 작성했습니다.파이썬 코딩 환경저는 윈도우10를 사용중이어서 최종적으로 miniconda + vscode를 사용하기로 했습니다.요즘 jupyter notebook는 ipynb 파일이 vscode에섴 확장프로그램으로 지원 되어서 굳이 설치 안해도 될거 같더라고요.설치 주소https://docs.conda.io/en/latest/miniconda.htmlhttps://code.visualstudio.com/ installed version miniconda 4.14.0 Python 3.9.12 vscode 1.71.2 miniconda를 설치할때 Windows Installer를 보시면 Python version이 적혀 있습니다. 저는 그냥 최신꺼 설치했는데 강의에서는 Python 3.8로 권장했습니다.miniconda나 vscode 설치할때 Path 옵션 설정해두시는게 편합니다. 저는 안했다가 윈도우 환경변수 설정에 추가했습니다.vscode를 킬때 꼭 miniconda 실행 후 작업 주소에서(base) [작업 주소]&gt;code .으로 진행해주시기 바랍니다. 아니면 Python 따로 설치하고 연결해야 합니다!저는 윈도우 터미널(wt.exe)를 추가적으로 설치해서 사용중입니다.윈도우에 있는 마이크로소프트 스토어 wt.exe 실행 시 아나콘다 실행 miniconda 시작 주소를 작업 주소로 설정이렇게 두면 실행 - wt.exe만으로 바로 작업에 돌입할 수 있습니다!TIP : 저는 레이저 게이밍 키보드의 매크로를 통해 hypershift키와 T 키를 조합하여 바로 터미널을 불러올 수 있습니다! 매크로가 사용 되는 키보드 또는 프로그램을 잘 이용해보세요.Pythonic Code코딩테스트는 Python으로 진행했지만 C로 작성하는 것 같이 문제를 풀었기에 이 부분부터 집중적으로 공부하게 되었습니다.split &amp; join split 문자열을 특정 기준값으로 나눕니다. s = \"1.2.3.4\" print(s.split('.')) &gt; [1 2 3 4] join 리스트의 문자열을 하나로 합칩니다. 특정 문자열로 연결됩니다. l = ['a', 'b', 'c', 'd'] print(''.join(l)) &gt; abcd print('-'.join(l)) &gt; a-b-c-d list comprehension리스트로 리스트를 만듭니다.일반적으로 for문보다 속도가 빠릅니다.case_1 = ['a', 'b', 'c']case_2 = ['d', 'e', 'f']print([i+j for i in case_1 for j in case_2])&gt; ['ad', 'ae', 'af', 'bd','be','bf']print([[i+j for i in case_1] for j in case_2])&gt;[['ad','bd','cd'], ['ae', 'be', 'ce'],['af', 'bf','cf']] enumerate &amp; zip enumerate list의 각 원소 앞에 index가 추가됩니다. for문에서 사용할때 유용합니다. l = ['a','b','c'] print(list(enumerate(l))) &gt; [(0,'a'), (1,'b'), (2,'c')] zip 두개의 list가 병렬로 합칩니다. case_1 = ['a', 'b', 'c'] case_2 = ['d', 'e', 'f'] print(list(zip(case_1, case_2))) &gt; [('a', 'd'), ('b','e'), ('c', 'f')] lambda &amp; map &amp; reduce lambda 이름을 지정하지 않는 함수 객체를 만드는 방법 f = lambda x,y : x+ y print(f(1,4)) &gt; 5 map 입력된 리스트의 원소 값을 각각 함수에 대입하여 출력합니다 f = lambda x,y : 2 * x + y print(list(map(f, [1,2,3],[4,5,6]))) &gt; [6, 9, 12] reduce 출력된 함수값을 다시 input으로 집어 넣습니다 import functools import reduce f = lambda x,y : 2 * x + y l = [1,2,3] print(reduce(f, l)) &gt; 11 # [1,2,3]이 입력되었을 때, x= 2, y=1이 입력 -&gt; 5 # 이후 출력된 5 =&gt; y, 남은 원소 3 =&gt; x 따라서 11 출력 # 헷갈리면 아래와 같이 쓰면 편하다 print(reduce(f, l[1:], l[0])) &gt; 11 # 앞에 있는 l[1:]의 원소들이 x로, 초기값 l[0]이후 출력한 함숫값이 y로 이동한다. iter &amp; generator iter sequence자료형(list, 튜플)을 순서대로 추출하는 개체 l = [1,2,3] iter_obj = iter(l) print(next(l)) &gt;1 print(next(iter_obj)) &gt;2 generator yield를 사용해 iter객체를 하나씩 반환합니다 무한 루프 또는 각 객체별 함수 실행 후 리턴해야할 경우 사용 특히 파일 관련 입출력이 있을 경우, 각 데이터에 대해서 처리하다가 오류가 나면 해당 데이터 전까지 저장하는 방식으로 사용이 가능합니다 AI의 특성상 빅데이터를 다루는 경우가 많기에 yield의 사용법은 필수적으로 알아야합니다 def generator_fun(my_list): yield from my_list my_list = [1,2,3] for num in my_list: time.sleep(1) print(num) #1초 뒤 &gt; 1 #1초 뒤 &gt; 2 #1초 뒤 &gt; 3 리스트 만들때처럼 ()로 generator 자료형 생성 가능 리스트보다 메모리를 적게 먹는다! gen_ex = (n*n for n in range(100)) 다항 변수 가변인자 (*) 개수가 정해지지 않은 함수의 매개변수를 지정할 수 있습니다. 각 함수별로 한 개만 맨 마지막에 사용할 수 있습니다. 남은 매개변수들이 tuple로 저장됩니다. def print_sum(fir = 10, *remain_args): print(fir + sum(remain_args)) # print_sum에 fir 초기값이 0으로 변경된 것을 알 수 있다 print_sum(0,1,2,3,4,5) &gt; 15 키워드 가변인자 (**) 함수에서 입력된 매개변수를 매개변수의 키워드와 함께 이루어진 딕셔너리로 사용할때 사용합니다. def p_info(**kwargs): for key, arg in kwargs.items(): print(key,' ', arg) p_info(name = 'rev', job = 'student') &gt; name rev &gt; job student asterisk (*) tuple, dict 자료형의 값들을 unpack 합니다. for data in zip(*([1,2],[3,4],[5,6])): print(data) &gt; (1, 3, 5) &gt; (2, 4, 6) 파이썬을 더 정리하고 싶었는데 AI math를 같이 들으면서 시간이 생각보다 부족하다는 것을 깨달았습니다.추후 강의를 들으면서 까먹은 Python 관련 내용이 있다면 복습하고 이 글을 업데이트 하겠습니다." }, { "title": "[AI 부캠 4기] Week0,1 후기", "url": "/posts/week1/", "categories": "Boostcamp, review", "tags": "Boostcamp, review", "date": "2022-09-22 00:00:00 +0900", "snippet": "부스트 캠프 0,1주차 간략 후기작성일은 목요일이지만 수업 및 과제와 퀴즈를 전부 완료하여 정리하는 시간을 가져보려고 합니다. 겸사겸사 마크다운도 연습해야죠.이번 AI 부스트캠프 4기부터는 시작 전 0주차부터 1주차 강의를 미리 수강할 수 있도록 배려해주어서 1주차에는 팀원과 친해지고 부스트캠프의 스케쥴에 익숙해질 수 있도록 시간을 배분할 수 있었습니다.1주차 강의는 이전 부스트캠프 프리코스에서 들었던 강좌들과 동일합니다. 다만 과제와 퀴즈를 풀면서 다시 공부할 수 밖에 없었네요. 특히 심화과제는 목요일이 된 지금까지도 답을 보지 않으면 풀지 못하고 있습니다.1주차의 시간표는 아래와 같습니다 일정 시간 데일리 스크럼 10:00 ~ 10:10 팀별 모두 각자 공부 시간(모각공) 10:10 ~ 11:00 개별 학습 활동 11:00 ~ 12:00 점심 시간 12:00 ~ 13:00 개별 학습 활동 13:00 ~ 16:00 피어 세션 16:00 ~ 17:00 개별 학습 활동 17:00 ~ 18:00 각 시간대별로 리뷰를 해보겠습니다. 참고로 저희 팀만의 그라운드 룰이 1주차부터 만들어져서 적용되어 있는 점 양해 바랍니다.0. 출석 체크데일리 스크럼 전 출석체크를 해야겠죠? 10시 10분 전까지 출석해야 합니다. 슬랙 출석체크 부스트코스 학습 시작 버튼 부스트코스 본인 인증어떻게보면 출석 시스템이 분할되어 비효율적으로 보이나 이게 부스트캠프 단독 진행이 아닌 kdt와 병행해서 진행하기 때문에 오류로 출석체크를 안해도 나머지 절차를 증거로 내기 위해 만든 시스템으로 이해 했습니다.여기에 저희 팀은 추가로 슬랙에서 자체 출석체크를 진행하고 있습니다.1. 데일리 스크럼데일리 스크럼은 0주차에 정해진 팀원들과 함께 오늘 할 공부를 정하는 시간입니다. 전날 피어세션에 참여 못한 팀원들을 위해 어제 했던 일들을 간략히 설명하기도 했습니다.목요일인 오늘 기준으로 과제를 팀원 전원이 피어세션 전까지 끝내고 이미 끝낸 사람들은 지금의 저처럼 학습했던 내용을 정리하자고 했습니다.2. 팀별 모두 각자 공부 시간데일리 스크럼으로 간략하게 설명했으면 이제 공부를 시작해야겠죠!줌을 필수적으로 켜서 함께 공부해야 합니다. 네이버에서 줌 회의실(pro 버전)을 제공하기 때문에 시간에 상관없이 접속이 가능합니다.가끔 운영진 분들이 오시는데 가볍게 인사하면 될 것 같습니다. 출석체크 확인용인것 같습니다.3. 개별 학습 활동점심시간 전후, 그리고 피어세션 후에 개별 학습 활동을 할 수 있습니다. 어떻게 보면 자유시간이지만 정말 자유시간처럼 보내면 진도를 따라잡지 못할 수 있습니다. 부스트캠프에 열정적이신 분들이 정말 많은 것 같아요. 2일차부터 심화 과제 관련 문의들을 봤습니다.저희 팀은 점심 이후로 줌을 켜고 개별 학습 활동을 진행하는 것으로 협의하였습니다. 사실상 모각공 시간과 동일하네요4. 피어세션아마 부스트캠프의 꽃이라고 하면 바로 이 피어세션을 의미하는 것 같습니다.팀이 모여서 금일 한 공부에 대해 리뷰하고 함께 탐색하며 친해질 수 있는 시간입니다. 1시간이지만 가장 배울 것이 많은 시간이라고 할 수 있네요.대부분의 그라운드 룰에 대한 토의도 이곳에서 이루어집니다.저는 모더레이터 역할도 겸임했는데, 수평적 구조인 부스트캠프에서 모더레이터란 회의록 기록 및 전체 회의 발표자를 의미합니다.다른 분들은 면접이나 타 팀에도 소속이 되어 있어서 바쁜데 저는 취업못한 백수기 때문에 제가 맡는 것이 당연하다고 생각되어서 맡았습니다.5. 특별 활동당연히 부스트 캠프가 위의 일정대로만 진행되는 것이 아닙니다. 1주차의 특별한 세션을 알아보겠습니다. 타운홀 미팅 (1일차)부스트 캠프의 OT입니다. 운영진분들과 멘토분들을 소개하고, 부스트캠프의 일정과 주의사항을 전달했습니다. 나이스투밋유 (1일차)이제 팀원들을 만나야겠지요?0주차 공지사항으로 정해진 팀원을 만나는 자리입니다. 간단한 자기소개를 시작으로 그라운드 룰을 만들고 3일차에 있는 [피어세션이 피었습니다] 를 위한 팀 소개 ppt를 제작해야 합니다. 피어세션이피었습니다 (3일차)팀원들을 만났다면 다른 팀들을 만날 차례입니다. 나이스트밋유 시간에 제작한 ppt로 발표를 진행합니다. 저희는 지명수배 종이처럼 꾸며서 현상금을 희망연봉으로 바꾸고 팀 이름도 BoostNet-14로 정해서 발표했었습니다. 마스터 클래스추후 작성 예정" }, { "title": "블로그 시작!!!", "url": "/posts/first/", "categories": "blog, talk", "tags": "Blog, Github Page", "date": "2022-09-21 00:00:00 +0900", "snippet": "Github 블로그 개설 완료! 네이버 AI 부스트 캠프 4기에 합격하였습니다!!!블로그를 초등학교 2학년을 마지막으로 더이상 사용해 본 적이 없는데부스트캠프 1주차를 진행하는 현재 제 학습 기록을 기록해야 할 필요성을 느꼈습니다.원래 velog, notion, Tistory 등 여러 개인 소장용 프로그램, 사이트를 둘러보고 있었는데, Github Page로 최종 결정하였습니다.GitHub page 선택 사유별 이유 없습니다. 무료!!!!!!! 아무래도 도메인 호스팅 가격이 비싼편은 아니어도 소형 블로그로 사용하는데 아까웠습니다.저는 개인적으로 구독제를 별로 좋아하지 않아 구독형 제품을 구매 잘 안합니다. 물론 xbox game pass처럼 새로운 컨텐츠가 나오는 구독형은 매우 좋다고 생각합니다. 하지만 동일한 기능에 추가적인 컨텐츠가 없는 제품들은 굳이 구독형이어야 할까요?유료가 아닌 무료로 사용이 가능한 플랫폼도 있습니다만 광고가 싫어 ADGUARD를 사용하는 저의 양심을 찌르기에 선택 안했습니다사실 Github의 무료 사용량인 5GB를 넘어버리면 유료로 전환을 해야 할 수도 있지만, 5GB를 넘기기 힘들뿐더러 넘더라면 jekyll 템플릿으로 만들어진 이 블로그를 다른 호스팅 사이트에 넘기면 될 것 같네요참고로 이 사이트의 댓글도 giscus를 사용하는데 github의 discussion을 블로그 채팅창처럼 쓸 수 있게 만든 아이디어를 보고 감탄하고 바로 넣었습니다.프론트엔드를 많이 다뤄보지 않아 마크다운도 어색한데 부스트캠프가 종료될 5개월 뒤에는 AI말고도 여러가지 배울 수 있었으면 하네요" } ]
