---
title:  "[AI 부캠 4기] week1 AI math 정리"
excerpt: "부캠 1주차 AI math 강의를 정리한 포스트입니다."

categories: [Boostcamp, study]
tags: [Boostcamp, AI, math]

toc: true
toc_sticky: true
toc_label : "목차"

pin: false

date: 2022-09-23
last_modified_at: 2022-09-23
render_with_liquid: true

---

<br>

1주차에 진행된 Python 강의에 이어 AI math를 정리해보려고 합니다.

역시 수학이 가장 힘들고 어려운 길이네요.

저는 수학을 싫어하지는 않지만 확률과 통계에 대한 이해가 매우 부족한 편입니다.

따라서 이 포스트를 보시는 분들은 잘못된 부분이 있을 수 있으니 조심하셔야 합니다.

부스트 캠프의 강의보단 제가 배우고 이해한 스토리대로 나가겠습니다.

---

# 왜 수학을 배우는가

---

## AI란?

`AI`란 무엇일까요?

저는 입력을 넣으면 원하는 결과가 나오는 `튜링머신`이 먼저 생각납니다.

사람의 생각이 튜링머신이라면 `AI`는 그 `튜링머신`을 따라가려는 존재입니다.

그런데 그 누구도 모든 사람의 생각을 코드로 짤 수 없습니다.

강아지와 고양이를 구별하는 일반적인 알고리즘이 존재하지 않는 것처럼 말이죠.

<br>

그러면 `AI`는 어떻게 사람의 생각을 따라갈까요?

사람의 생각 또한 `학습`의 결과이기 때문에 `AI` 또한 `학습`으로 발전합니다.

수많은 데이터를 보고 학습하여 `튜링머신`과 비슷한 결과 값을 내도록 말이죠.

<br>

## 학습이란?

그러면 `학습`이란 무엇일까요?

저는 부모님과 선생님, 교수님들을 통해 이전의 역사를 외우고 익혔습니다.

컴퓨터에서 `학습`이란 입력된 데이터들의 결과값이 누적되어 다음 입력 데이터에 대해서 누적된 결과값을 기반으로 해석, 원하던 결과를 내보낼 수 있게 하는 수단이라고 생각합니다.

여기에서 `AI`의 `학습`에 대해 여러가지 방법이 있습니다.

- 지도 학습 : 

    정답이 정해져 있는 학습
    
    문제에 대한 답이 정해져 있어서 실행 결과와 정답의 `오차`를 비교

- 비지도 학습:

    정답이 없는 학습

    데이터의 규칙성을 찾는다

- 강화 학습:

    상태가 주어지는 학습

    사람이 시행착오를 겪으며(정답이 없음) 학습하는 것과 유사

<br>


지도 학습 뿐만 아니라 다른 학습들의 조건들을 보면 컴퓨터 코드로 적기 애매한 부분만 있습니다.

여기서 제가 주목한 부분은 `오차` 입니다. `오차`란 수치로 표현될 수 있는 값입니다.

컴퓨터는 결국 모든 것을 0과 1로 인식하기 때문에 사진, 문자열, 상태 등 모든 것을 숫자로 인식합니다.

그렇다면 `학습`의 결과물과 사람이 원하는 결과물 또한 숫자로 인식할 수 있습니다.

즉 `오차`를 구하기 위해선 숫자의 나열로 된두 결과값의 차이, 즉 두 `벡터`의 거리를 계산하면 됩니다. l1-norm, l2-norm 등의 계산법이 있습니다.

{% raw %}
$$
||x||_n = (\sum^n_{i=1}|x_i|^p)^{1/p}
$$
{% endraw %}

두 결과물의 차이를 `오차`라고 둔다면 `AI`의 최종 목표는 이 `오차`를 최소화 하는 것이라고 볼 수 있겠습니다.

<br>

## 오차는 어떻게 최소화 하는가?

오차를 그러면 어떻게 최소화 할까요?

여러 데이터를 넣다 보면 오차가 아래 처럼 그려질 수도 있습니다.

그래프에서 X는 우리가 입력할 데이터, Y는 `오차`라고 합시다.

![y=(x-4)^2+10](week1/graph1.png){:style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;" }

{% raw %}
$$
y = (x-4) ^ 2 + 10
$$
{% endraw %}

위의 수식에서 우리는 x가 4일때 `최솟값`이 10이고 라는 것을 알 수 있습니다.

하지만 수식을 모른다고 할때, 그리고 실수는 무한하기에 그래프의 전체 모양을 잡는다던가 특정 범위의 그래프만 그릴 수는 없습니다.

그러면 `최솟값`이 되는 x를 찾기 위해서는 어떻게 할까요? 운 없어서 10이라는 x값을 넣고도 4라는 x를 도출해 낼 수 있을까요?



![y=(x-4)^2+10](week1/graph2.png){:style="border:1px solid #eaeaea; border-radius: 7px; padding: 0px;" }

{% raw %}
$$
y' = 2(x-4)
$$
{% endraw %}

고등학교에서 배운 미분을 사용해보면 x = 4에서 `미분값`이 0이라는 것을 알 수 있습니다.

그리고 x = 10 에서 미분값을 구하게 된다면 12라는 양수가 나오게 되지요.

즉 `미분값`이 0이 되도록 x를 이동시키면 되는 것입니다!

이를 `경사하강법`이라고 합니다. 경사를 계속 낮추어서 0에 가깝게 만드는 것이죠.

`경사하강법`에서 x에 대한 `미분값`을 `그래디언트`라고 합니다.

<br>

## 경사하강법

`경사하강법`은 코드로 작성하면 아래와 같습니다.

```python
# gradient : 미분을 계산하는 함수
# init : 시작점 , lr: 학습률, eps : 알고리즘 종료 조건

var = init
grad = gradient(var)
while(abs(grad) > eps):
    var = var - lr * grad
    grad = gradient(var)
```

수학보다는 역시 코드가 더 보기 편하네요

한눈에 보기 어렵지만 `그래디언트`과 `학습률`를 곱한 것으로 입력값을 변경해서 찾아내는 것입니다. 

`학습률`이 높으면 더 빨리 찾을 수 있지만 너무 큰 값은 최소점을 지나칠 수 있기 떄문에 적당한 `학습률`을 사용해야 합니다.

`종료조건`은 왜 필요할까요? 위 그래프처럼 `그래디언트`가 0이 되는 위치가 정수 또는 유한 소수면 문제 없겠지만 무한 소수라면 컴퓨터의 한계로 인해 절대 `그래디언트`이 0이 되는 x를 찾을 수 없습니다.

<br>

- 변수가 벡터인 경우

    `오차`를 이야기 할때 `벡터` 이야기를 꺼낸적이 있었는데, `벡터`에 해당하는   그래프를 `다항 함수`로 해석할 수 있습니다.


    {% raw %}
    $$
    f(x, y, z) = a(x-4)^2 + b(y-2)^2 + c(z-3)^2 + d
    $$
    {% endraw %}


    보다시피 x = 4, y = 2, z = 3일때 최솟값 d가 나오는 것을 알 수 있습니다.

    `경사하강법`을 적용 하려면 어떻게 해야 할까요?

    x에 대한 최솟값을 위해 x로 미분, y에 대한 최솟값을 위해 y로 미분...

    <br>

    즉 각 원소에 대해 함수를 미분하여 `경사하강법`을 적용하면 됩니다.

    이러한 미분을 `편미분`이라고 하며, 편미분한 함수가 모인 `벡터`를 `그래디언트    벡터`라고 합니다.


    ```
    var = var - lr * grad
    ```


    아까전 코드에서 lr을 이제 변수인 `벡터`로 대응하고 grad를 `그래디언트   벡터`로 대체하면 `경사하강법`이 동일하게 동작하겠죠? 그래서 아래와 같은   코드로 표현이 됩니다.

    ```python
    # gradient : 미분을 계산하는 함수
    # init : 시작점 , lr: 학습률, eps : 알고리즘 종료 조건

    var = init
    grad = gradient(var)
    while(norm(grad) > eps):
        var = var - lr * grad
        grad = gradient(var)
    ```

    상단의 코드와 동일하지만 `while(norm(grad) > eps):`이라는 부분이    달라졌습니다. 벡터에서 거리는 `ln-norm`으로 측정하기 때문입니다. 처음에    이야기 했었죠?

## 선형 회귀 분석
